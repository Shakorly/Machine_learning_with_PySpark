{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0HJaUnjF5P/WduI6eniqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shakorly/Machine_learning_with_PySpark/blob/main/overview_of_Machine_learning_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 1: MLlib**\n",
        "\n",
        "Welcome to the \"Machine Learning with PySpark\" tutorial! In this first chapter, we'll introduce you to the core library we'll be using for building machine learning models on large datasets: MLlib.\n",
        "\n",
        "Imagine you have a huge collection of something, like information about millions of houses across a country, and you want to predict the price of a new house based on its features (like size, location, number of rooms). Doing this with traditional machine learning tools might be slow or even impossible on a single computer if the dataset is too big to fit into memory.\n",
        "\n",
        "This is where Spark comes in. Spark is a powerful engine designed to process very large datasets by distributing the work across many computers in a cluster.\n",
        "\n",
        "Now, within this powerful Spark engine, we need specific tools for machine learning tasks like predicting prices (regression), classifying emails as spam or not spam (classification), or grouping similar customers together (clustering). This is exactly what MLlib provides.\n",
        "\n",
        "Think of Spark as a large, well-equipped workshop capable of handling enormous projects. MLlib is the specialized toolbox within that workshop specifically designed for machine learning projects. It contains all the hammers, saws, and specific gadgets you need for tasks like training models, preparing your data for training, and making predictions, all built to work seamlessly with Spark's ability to handle big data.\n",
        "\n",
        "**What is MLlib?**\n",
        "MLlib is Spark's Machine Learning Library. Its primary goal is to provide a scalable and easy-to-use set of machine learning algorithms and tools that can run on large datasets distributed across a cluster of computers.\n",
        "\n",
        "Instead of trying to load all your massive data onto one machine's memory, MLlib algorithms are designed to process data in chunks, in parallel, on different machines managed by Spark. This is the key to handling \"big data\" for machine learning.\n",
        "\n",
        "MLlib offers tools for various common machine learning tasks, such as:\n",
        "\n",
        "Classification: Categorizing data (e.g., is this a picture of a cat or a dog? Is this transaction fraudulent?).\n",
        "Regression: Predicting a continuous value (e.g., what will the price of this house be? How much will this customer spend?).\n",
        "Clustering: Grouping similar data points together (e.g., finding different customer segments).\n",
        "Collaborative Filtering: Making recommendations (e.g., suggesting movies based on what other users liked).\n",
        "Feature Extraction & Transformation: Preparing your data for the machine learning algorithms.\n",
        "For our house price prediction example, we would use MLlib's regression algorithms.\n",
        "\n",
        "**Why Use MLlib?**\n",
        "You might already know about other great machine learning libraries like scikit-learn, TensorFlow, or PyTorch. These are excellent, but they are primarily designed to run on a single machine, possibly using multiple cores or GPUs on that machine.\n",
        "\n",
        "When your dataset grows beyond what a single machine can handle, you need a distributed solution. MLlib, built on Spark, is designed precisely for this scenario. It allows you to use familiar machine learning concepts and algorithms but execute them on data spread across an entire cluster of machines.\n",
        "\n",
        "Here's a simple comparison:"
      ],
      "metadata": {
        "id": "MC2XVAW8-Wjw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrCvnUTx8IL8",
        "outputId": "bac3ed48-d25a-4961-e760-0997d54089cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully insatll SparkSession\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "print(\"Successfully insatll SparkSession\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code simply shows that you access MLlib's capabilities by importing specific tools or algorithms from the pyspark.ml package. When you install pyspark, you get MLlib automatically."
      ],
      "metadata": {
        "id": "2NALyaMF_m9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLlib**:\n",
        "The Starting Point\n",
        "MLlib itself is the collection of algorithms and helper functions. However, to use MLlib, you first need to interact with Spark. The entry point for any Spark application, including those using MLlib, is something called a SparkSession.\n",
        "\n",
        "The SparkSession is like getting the keys to the Spark workshop. Once you have it, you can load data and start using the tools (MLlib) within it.\n",
        "\n",
        "We will explore how to get started with Spark and how to obtain this crucial SparkSession in the next chapter.\n",
        "\n",
        "Conclusion\n",
        "In this chapter, we learned that MLlib is Spark's powerful library for doing machine learning on big, distributed datasets. It provides algorithms and tools for common ML tasks like regression and classification, designed to work in parallel across a cluster of machines. We saw that MLlib is accessed via the pyspark.ml package and operates by distributing computation alongside Spark's data distribution.\n",
        "\n",
        "To start using MLlib, the very first step is always to set up a Spark environment and get a way to interact with it. This is where the SparkSession comes in.\n",
        "\n",
        "Ready to open the Spark workshop? Let's learn about the SparkSession in the next chapter."
      ],
      "metadata": {
        "id": "UhGDt9yZErma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create a SparkSession\n",
        ">> Creating a SparkSession is usually the very first step in any PySpark application. You typically use a builder pattern. Here's the basic way to do it:"
      ],
      "metadata": {
        "id": "0xyhwivhqHzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"MyFirstSparkApp\").getOrCreate()"
      ],
      "metadata": {
        "id": "JqGAbYSU_fl7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pqgn4aH4rO7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's break down this simple code:\n",
        "\n",
        ">>from pyspark.sql import SparkSession: This line imports the necessary class from the PySpark library.\n",
        "\n",
        "\n",
        "\n",
        ">>SparkSession.builder: This starts the process of building a SparkSession object. It returns a builder object that helps configure the session.\n",
        "\n",
        ">>.appName(\"MyFirstSparkApp\"): You give your Spark application a name. This is helpful when you're running many applications on a cluster; you can easily identify yours in Spark's monitoring UIs. Choose a descriptive name!\n",
        "\n",
        ">>.getOrCreate(): This is the key method. It checks if a SparkSession already exists with the current configuration.\n",
        "If one exists, it returns the existing session.\n",
        "\n",
        "--------------------------------------\n",
        "If one does not exist, it creates a new one based on the configurations you've set (like the app name).\n",
        "This pattern ensures you don't accidentally create multiple SparkSessions in the same application, which is usually not desired."
      ],
      "metadata": {
        "id": "qydgHWzqrOIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let read in our first data\n",
        "\n",
        "we work with this  Automotive Data through out the project"
      ],
      "metadata": {
        "id": "bFSDB9nxrsvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('/content/data.csv', inferSchema=True, header=True, sep=\",\")\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kji5mi_kqaMb",
        "outputId": "369fcbb0-7711-4b0d-83d2-49fb83f1fdd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------+-----------+---------+----------+------------+-----------+------------+---------------+----------+------+-----+------+-----------+-----------+----------------+-----------+-----------+----+------+-----------------+----------+--------+--------+-----------+-----+\n",
            "|symboling|normalized-losses|       make|fuel-type|aspiration|num-of-doors| body-style|drive-wheels|engine-location|wheel-base|length|width|height|curb-weight|engine-type|num-of-cylinders|engine-size|fuel-system|bore|stroke|compression-ratio|horsepower|peak-rpm|city-mpg|highway-mpg|price|\n",
            "+---------+-----------------+-----------+---------+----------+------------+-----------+------------+---------------+----------+------+-----+------+-----------+-----------+----------------+-----------+-----------+----+------+-----------------+----------+--------+--------+-----------+-----+\n",
            "|        3|             NULL|alfa-romero|      gas|       std|         two|convertible|         rwd|          front|      88.6| 168.8| 64.1|  48.8|       2548|       dohc|            four|        130|       mpfi|3.47|  2.68|              9.0|       111|    5000|      21|         27|13495|\n",
            "|        3|             NULL|alfa-romero|      gas|       std|         two|convertible|         rwd|          front|      88.6| 168.8| 64.1|  48.8|       2548|       dohc|            four|        130|       mpfi|3.47|  2.68|              9.0|       111|    5000|      21|         27|16500|\n",
            "|        1|             NULL|alfa-romero|      gas|       std|         two|  hatchback|         rwd|          front|      94.5| 171.2| 65.5|  52.4|       2823|       ohcv|             six|        152|       mpfi|2.68|  3.47|              9.0|       154|    5000|      19|         26|16500|\n",
            "|        2|              164|       audi|      gas|       std|        four|      sedan|         fwd|          front|      99.8| 176.6| 66.2|  54.3|       2337|        ohc|            four|        109|       mpfi|3.19|   3.4|             10.0|       102|    5500|      24|         30|13950|\n",
            "|        2|              164|       audi|      gas|       std|        four|      sedan|         4wd|          front|      99.4| 176.6| 66.4|  54.3|       2824|        ohc|            five|        136|       mpfi|3.19|   3.4|              8.0|       115|    5500|      18|         22|17450|\n",
            "+---------+-----------------+-----------+---------+----------+------------+-----------+------------+---------------+----------+------+-----+------+-----------+-----------+----------------+-----------+-----------+----+------+-----------------+----------+--------+--------+-----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   spark.read: Accesses the reader object within the SparkSession.\n",
        "*   .csv:Meaning the data is a csv format\n",
        "NOTE: Ypu can also pass another data format, like json, parguet, exel ect\n",
        "\n",
        "*   inferSchema=True: Asks Spark to try and automatically figure out the data types for each column (like 'age' being an integer). For large datasets, it's often better practice to define the schema manually for performance, but this is easy for beginners.\n",
        "\n",
        "*   header=True: Indicates that the first row of the CSV is a header row containing column names.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e86J32EwwpBB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrBqkSwst863"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}